{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66ba47ed-189e-4769-b318-fd4da05bc6fa",
   "metadata": {},
   "source": [
    "Step 1: Install langchain and ollama\n",
    "\n",
    "1. pip install langchain\n",
    "2. download ollama from https://ollama.ai/download\n",
    "\n",
    "Step 2: Donwload a model \n",
    "\n",
    "1. Make sure ollama server is running -> ollama serve\n",
    "2. Fetch a model -> ollama pull llama2\n",
    "\n",
    "Step 3: Invoke the model in your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8659ff7c-9bc5-43b8-a12b-1ba11f9c2b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c8b7b91-664d-4083-9354-119ce2632dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running llama2 locally on your device can provide several benefits, including:\n",
      "\n",
      "1. Faster Performance: By running llama2 locally, you can avoid any potential latency or bandwidth issues that may arise when using a remote service. This can result in faster performance and a more responsive experience.\n",
      "2. Increased Security: Running llama2 locally can help ensure that your data remains private and secure, as you don't have to transmit it over the internet to a remote server. This can be particularly important if you're working with sensitive or confidential information.\n",
      "3. More Control: When you run llama2 locally, you have more control over the configuration and settings of the service. You can customize it to your specific needs and ensure that it's optimized for your device and network environment.\n",
      "4. Reduced Dependence on Internet Connectivity: Running llama2 locally eliminates the need for a reliable internet connection, which can be helpful in areas with unreliable or limited internet access. This can also be beneficial for users who want to work offline or in environments where internet connectivity is not allowed.\n",
      "5. Better Compatibility: Running llama2 locally can help ensure better compatibility with your device and its operating system, as you don't have to worry about compatibility issues that may arise when using a remote service.\n",
      "6. Customization: When running llama2 locally, you have more flexibility to customize the service to your specific needs. You can tailor it to work with your preferred software, hardware, and network settings.\n",
      "7. Cost-Effective: Running llama2 locally can be a cost-effective solution, as you don't have to pay for remote hosting or bandwidth fees. This can be particularly beneficial for users who are working on a tight budget or who want to save money on their workflow.\n",
      "8. Improved Reliability: Running llama2 locally can help ensure that the service is more reliable and less prone to downtime or errors, as you have more control over the underlying infrastructure and configuration.\n",
      "9. Better Debugging: When running llama2 locally, it's easier to debug and troubleshoot issues, as you have direct access to the service and its underlying components. This can help you identify and fix problems more quickly and efficiently.\n",
      "10. More Flexibility: Running llama2 locally provides more flexibility in terms of scalability and customization. You can easily scale the service up or down based on your needs, and you have more control over the underlying infrastructure and configuration.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"What are the benefits of running llama2 locally on your device?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d9ef5-c06a-4d3c-8418-00243dd804cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
