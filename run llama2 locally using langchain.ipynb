{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8659ff7c-9bc5-43b8-a12b-1ba11f9c2b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c8b7b91-664d-4083-9354-119ce2632dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running LLaMA locally on your device can provide several benefits, including:\n",
      "\n",
      "1. Faster Response Time: By running LLaMA locally, you can reduce the latency associated with making API calls to a remote server. This can result in faster response times and a smoother user experience.\n",
      "2. Improved Security: Running LLaMA locally can help protect your data from potential security threats that may arise during transmission over the internet. By keeping the model on your device, you can ensure that your data stays local and is not exposed to potential hackers or cybercriminals.\n",
      "3. Better Performance: Running LLaMA locally can also improve the performance of your application. By processing the text within your device, you can reduce the amount of data that needs to be transmitted over the internet, resulting in faster and more efficient processing times.\n",
      "4. Offline Accessibility: With LLaMA running locally, you can access the model offline, without needing an internet connection. This can be particularly useful for applications that require AI capabilities but do not have reliable internet access.\n",
      "5. Customization: Running LLaMA locally gives you greater control over the model's behavior and performance. You can fine-tune the model to meet your specific needs and customize its behavior to suit your application.\n",
      "6. Cost-Effective: Depending on the scale of your application, running LLaMA locally may be more cost-effective than relying on a remote server. By keeping the model on your device, you can avoid paying for expensive cloud computing resources or bandwidth fees.\n",
      "7. Better Integration: Running LLaMA locally can enable better integration with other applications and services on your device. By having direct access to the model, you can more easily incorporate AI capabilities into your existing application ecosystem.\n",
      "8. Customization for Edge Devices: With LLaMA running locally, you can customize the model's behavior to optimize its performance on edge devices with limited computing resources. This can help ensure a smooth user experience even on lower-powered devices.\n",
      "9. Better Compliance: Depending on your industry or application, there may be regulatory requirements for data storage and processing. Running LLaMA locally can help you better comply with these regulations by keeping the model and its associated data within your control.\n",
      "10. Increased Control: By running LLaMA locally, you have greater control over the model's behavior and performance. You can monitor its activity, update it more frequently, or make changes to its behavior as needed.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"What are the benefits of running llama2 locally on your device?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d9ef5-c06a-4d3c-8418-00243dd804cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
